{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select GPU [0 or 1]: 0\n"
     ]
    }
   ],
   "source": [
    "# From: https://github.com/keras-team/keras/issues/6031\n",
    "import os\n",
    "gpu_id = input( \"Select GPU [0 or 1]: \" )\n",
    "\n",
    "if gpu_id in [ \"0\", \"1\" ]:\n",
    "    os.environ[ \"CUDA_VISIBLE_DEVICES\" ] = gpu_id\n",
    "else:\n",
    "    print( \"Invalid GPU id.  Defaulting to '0,1'\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose CPU Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Share CPU cores w/ other models? [y/n]: y\n",
      "Allocating 6 cores to this notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "cores = 12\n",
    "share_cores = input( \"Share CPU cores w/ other models? [y/n]: \" )\n",
    "\n",
    "if share_cores == \"y\":\n",
    "    \n",
    "    cores = cores / 2\n",
    "\n",
    "print( \"Allocating %d cores to this notebook\" % cores )\n",
    "\n",
    "# From: https://stackoverflow.com/questions/46421258/limit-number-of-cores-used-in-keras\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(\n",
    "    K.tf.Session(\n",
    "        config=K.tf.ConfigProto(\n",
    "            intra_op_parallelism_threads=32, inter_op_parallelism_threads=32 \n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018.06.03 14:37\n",
      "Time to process: [1] seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import datetime\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, r2_score\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# My local install of jupyter notebook has broken matplotlib notebook and inline image rendering. Fix later! \n",
    "# from matplotlib import figure\n",
    "# f = figure.Figure( figsize=( 10, 6 ) )\n",
    "# #plt.figure( figsize=( 10, 6 ) )\n",
    "\n",
    "cores = 12\n",
    "\n",
    "def get_time( output=True ):\n",
    "    \n",
    "    temp = time.time()\n",
    "    if output:\n",
    "        now = datetime.datetime.now()\n",
    "        print( now.strftime( \"%Y.%m.%d %H:%M\" ) )\n",
    "        \n",
    "    return temp\n",
    "\n",
    "foo = get_time()\n",
    "\n",
    "def print_time( start_time, end_time, interval=\"seconds\" ):\n",
    "    \n",
    "    if interval == \"hours\":\n",
    "        print ( \"Time to process: [%s] hours\" % ( str( ( end_time - start_time ) / 60 / 60 ) ) )\n",
    "    else:\n",
    "        print ( \"Time to process: [%s] seconds\" % ( str( end_time - start_time ) ) )\n",
    "\n",
    "print_time( 0, 1 )\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018.06.03 14:37\n",
      "2018.06.03 14:37\n",
      "Time to process: [3.1850333213806152] seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendorid</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>ratecodeid</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>...</th>\n",
       "      <th>payment_type_tip_bin_125_enc</th>\n",
       "      <th>ratecode_tip_bin_125_enc</th>\n",
       "      <th>payment_type_tip_bin_126_enc</th>\n",
       "      <th>ratecode_tip_bin_126_enc</th>\n",
       "      <th>payment_type_tip_bin_127_enc</th>\n",
       "      <th>ratecode_tip_bin_127_enc</th>\n",
       "      <th>payment_type_tip_bin_128_enc</th>\n",
       "      <th>ratecode_tip_bin_128_enc</th>\n",
       "      <th>payment_type_tip_bin_129_enc</th>\n",
       "      <th>ratecode_tip_bin_129_enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>-73.979485</td>\n",
       "      <td>40.684956</td>\n",
       "      <td>-73.979431</td>\n",
       "      <td>40.685020</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>-74.010796</td>\n",
       "      <td>40.912216</td>\n",
       "      <td>-74.010780</td>\n",
       "      <td>40.912212</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.921410</td>\n",
       "      <td>40.766708</td>\n",
       "      <td>-73.914413</td>\n",
       "      <td>40.764687</td>\n",
       "      <td>1</td>\n",
       "      <td>0.59</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.921387</td>\n",
       "      <td>40.766678</td>\n",
       "      <td>-73.931427</td>\n",
       "      <td>40.771584</td>\n",
       "      <td>1</td>\n",
       "      <td>0.74</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.955482</td>\n",
       "      <td>40.714046</td>\n",
       "      <td>-73.944412</td>\n",
       "      <td>40.714729</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 305 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   vendorid  store_and_fwd_flag  ratecodeid  pickup_longitude  \\\n",
       "0         2               False           5        -73.979485   \n",
       "1         2               False           5        -74.010796   \n",
       "2         2               False           1        -73.921410   \n",
       "3         2               False           1        -73.921387   \n",
       "4         2               False           1        -73.955482   \n",
       "\n",
       "   pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \\\n",
       "0        40.684956         -73.979431         40.685020                1   \n",
       "1        40.912216         -74.010780         40.912212                1   \n",
       "2        40.766708         -73.914413         40.764687                1   \n",
       "3        40.766678         -73.931427         40.771584                1   \n",
       "4        40.714046         -73.944412         40.714729                1   \n",
       "\n",
       "   trip_distance  fare_amount            ...             \\\n",
       "0           0.00          7.8            ...              \n",
       "1           0.00         45.0            ...              \n",
       "2           0.59          4.0            ...              \n",
       "3           0.74          5.0            ...              \n",
       "4           0.61          5.0            ...              \n",
       "\n",
       "   payment_type_tip_bin_125_enc  ratecode_tip_bin_125_enc  \\\n",
       "0                      0.000086                  0.000000   \n",
       "1                      0.000086                  0.000000   \n",
       "2                      0.000086                  0.000041   \n",
       "3                      0.000000                  0.000041   \n",
       "4                      0.000000                  0.000041   \n",
       "\n",
       "   payment_type_tip_bin_126_enc  ratecode_tip_bin_126_enc  \\\n",
       "0                      0.000006                  0.000000   \n",
       "1                      0.000006                  0.000000   \n",
       "2                      0.000006                  0.000003   \n",
       "3                      0.000000                  0.000003   \n",
       "4                      0.000000                  0.000003   \n",
       "\n",
       "   payment_type_tip_bin_127_enc  ratecode_tip_bin_127_enc  \\\n",
       "0                      0.000013                  0.000000   \n",
       "1                      0.000013                  0.000000   \n",
       "2                      0.000013                  0.000006   \n",
       "3                      0.000000                  0.000006   \n",
       "4                      0.000000                  0.000006   \n",
       "\n",
       "   payment_type_tip_bin_128_enc  ratecode_tip_bin_128_enc  \\\n",
       "0                      0.000016                  0.000000   \n",
       "1                      0.000016                  0.000000   \n",
       "2                      0.000016                  0.000008   \n",
       "3                      0.000000                  0.000008   \n",
       "4                      0.000000                  0.000008   \n",
       "\n",
       "   payment_type_tip_bin_129_enc  ratecode_tip_bin_129_enc  \n",
       "0                      0.000019                  0.000000  \n",
       "1                      0.000019                  0.000000  \n",
       "2                      0.000019                  0.000009  \n",
       "3                      0.000000                  0.000009  \n",
       "4                      0.000000                  0.000009  \n",
       "\n",
       "[5 rows x 305 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_start = get_time()\n",
    "#trips = pd.read_csv( \"data/green-tripdata-2015-09-cleaned.csv\" )\n",
    "trips = pd.read_parquet( \"data/green-tripdata-2015-09-cleaned-plus-experimental-features.parquet\" )\n",
    "print_time( page_start, get_time() )\n",
    "trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455831, 305)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train & test sets: omit tip or related values\n",
    "#X_trips = trips.drop( columns=[ \"tip_percent\", \"tip_amount\", \"total_amount\" ] )\n",
    "X_trips = trips.drop( columns=[ \"tip_percent\", \"tip_amount\", \"total_amount\", \"tip_recorded\", \"tip_percent_bin\" ] )\n",
    "y_trips = trips.tip_percent\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split( X_trips, y_trips, random_state=42 )\n",
    "# eval_set = [ ( X_test, y_test ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_count = X_trips.shape[ 1 ]\n",
    "feature_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "\n",
    "def baseline_model():\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add( Dense( feature_count, input_dim=feature_count, kernel_initializer='normal', activation='relu' ) )\n",
    "    model.add( Dense( 1, kernel_initializer='normal' ) )\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam' )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed( seed )\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor( build_fn=baseline_model, epochs=5, batch_size=512, verbose=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1164664/1164664 [==============================] - 67s 58us/step - loss: 63.5557\n",
      "Epoch 2/5\n",
      "1164664/1164664 [==============================] - 67s 57us/step - loss: 54.4772\n",
      "Epoch 3/5\n",
      "1164664/1164664 [==============================] - 67s 57us/step - loss: 54.1084\n",
      "Epoch 4/5\n",
      "1164664/1164664 [==============================] - 67s 57us/step - loss: 53.8861\n",
      "Epoch 5/5\n",
      "1164664/1164664 [==============================] - 67s 57us/step - loss: 53.7354\n",
      "291167/291167 [==============================] - 2s 6us/step\n",
      "Epoch 1/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 61.0054\n",
      "Epoch 2/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 54.7402\n",
      "Epoch 3/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 54.4325\n",
      "Epoch 4/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 54.0627\n",
      "Epoch 5/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 53.9845\n",
      "291166/291166 [==============================] - 2s 6us/step\n",
      "Epoch 1/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 62.3578\n",
      "Epoch 2/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 54.4093\n",
      "Epoch 3/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 53.8927\n",
      "Epoch 4/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 53.7928\n",
      "Epoch 5/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 53.6073\n",
      "291166/291166 [==============================] - 2s 6us/step\n",
      "Epoch 1/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 61.4227\n",
      "Epoch 2/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 54.3116\n",
      "Epoch 3/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 54.0297\n",
      "Epoch 4/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 53.8056\n",
      "Epoch 5/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 53.6359\n",
      "291166/291166 [==============================] - 2s 6us/step\n",
      "Epoch 1/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 61.1913\n",
      "Epoch 2/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 54.3096\n",
      "Epoch 3/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 53.9968\n",
      "Epoch 4/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 53.7762\n",
      "Epoch 5/5\n",
      "1164665/1164665 [==============================] - 67s 58us/step - loss: 53.6483\n",
      "291166/291166 [==============================] - 2s 6us/step\n",
      "Baseline results: -53.64 (0.68) MSE\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold( n_splits=5, random_state=seed )\n",
    "results = cross_val_score( estimator, X_trips, y_trips, cv=kfold )\n",
    "print( \"Baseline results: %.2f (%.2f) MSE\" % ( results.mean(), results.std() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline results: -53.64 (0.68) MSE\n"
     ]
    }
   ],
   "source": [
    "print( \"Baseline results: %.2f (%.2f) MSE\" % ( results.mean(), results.std() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1164664/1164664 [==============================] - 4s 3us/step - loss: 62.9994\n",
      "Epoch 2/5\n",
      "1164664/1164664 [==============================] - 3s 3us/step - loss: 52.0922\n",
      "Epoch 3/5\n",
      "1164664/1164664 [==============================] - 3s 3us/step - loss: 51.8044\n",
      "Epoch 4/5\n",
      "1164664/1164664 [==============================] - 3s 3us/step - loss: 51.6528\n",
      "Epoch 5/5\n",
      "1164664/1164664 [==============================] - 3s 3us/step - loss: 51.5538\n",
      "291167/291167 [==============================] - 1s 2us/step\n",
      "Epoch 1/5\n",
      "1164665/1164665 [==============================] - 4s 3us/step - loss: 63.3961\n",
      "Epoch 2/5\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 52.4526\n",
      "Epoch 3/5\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 52.1518\n",
      "Epoch 4/5\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 52.0119\n",
      "Epoch 5/5\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 51.8958\n",
      "291166/291166 [==============================] - 1s 2us/step\n",
      "Epoch 1/5\n",
      "1164665/1164665 [==============================] - 4s 3us/step - loss: 62.4839\n",
      "Epoch 2/5\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 52.1276\n",
      "Epoch 3/5\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 51.8512\n",
      "Epoch 4/5\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 51.7000\n",
      "Epoch 5/5\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 51.6046\n",
      "291166/291166 [==============================] - 1s 2us/step\n",
      "Epoch 1/5\n",
      "1164665/1164665 [==============================] - 4s 3us/step - loss: 62.6962\n",
      "Epoch 2/5\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 52.1178\n",
      "Epoch 3/5\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 51.8324\n",
      "Epoch 4/5\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 51.7099\n",
      "Epoch 5/5\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 51.6351\n",
      "291166/291166 [==============================] - 0s 2us/step\n",
      "Epoch 1/5\n",
      "1164665/1164665 [==============================] - 4s 3us/step - loss: 62.5494\n",
      "Epoch 2/5\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 52.1058\n",
      "Epoch 3/5\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 51.8113\n",
      "Epoch 4/5\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 51.6622\n",
      "Epoch 5/5\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 51.6084\n",
      "291166/291166 [==============================] - 1s 2us/step\n",
      "Standardized results: -51.74 (0.47) MSE\n"
     ]
    }
   ],
   "source": [
    "# evaluate using standarized input values\n",
    "np.random.seed( seed )\n",
    "estimators = []\n",
    "estimators.append( ( 'standardize', StandardScaler() ) )\n",
    "estimators.append( ( 'mlp', KerasRegressor( build_fn=baseline_model, epochs=5, batch_size=512, verbose=True ) ) )\n",
    "pipeline = Pipeline( estimators )\n",
    "kfold = KFold( n_splits=5, random_state=seed )\n",
    "results_scaled = cross_val_score( pipeline, X_trips, y_trips, cv=kfold )\n",
    "print( \"Standardized results: %.2f (%.2f) MSE\" % ( results_scaled.mean(), results_scaled.std() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized results: -51.74 (0.47) MSE\n"
     ]
    }
   ],
   "source": [
    "print( \"Standardized results: %.2f (%.2f) MSE\" % ( results_scaled.mean(), results_scaled.std() ) )\n",
    "# print( \"Standardized: mean [%.2f], std [%.2f], max [%.2f], and min[%.2f] RMSE\" % ( np.sqrt( results_scaled.mean() ), np.sqrt( results_scaled.std() ), np.sqrt( results_scaled.max() ), np.sqrt( results_scaled.min() ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create deeper model\n",
    "def larger_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add( Dense( feature_count, input_dim=feature_count, kernel_initializer='normal', activation='relu' ) )\n",
    "    # half as many nodes as features as above\n",
    "    model.add( Dense( feature_count // 2, kernel_initializer='normal', activation='relu' ) )\n",
    "    model.add( Dense( 1, kernel_initializer='normal' ) )\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam' )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 46.5948\n",
      "Epoch 2/5\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.9705\n",
      "Epoch 3/5\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.7440\n",
      "Epoch 4/5\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.6607\n",
      "Epoch 5/5\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.6083\n",
      "291167/291167 [==============================] - 0s 1us/step\n",
      "Epoch 1/5\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 49.0822\n",
      "Epoch 2/5\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 25.2280\n",
      "Epoch 3/5\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.8287\n",
      "Epoch 4/5\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.6927\n",
      "Epoch 5/5\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.6230\n",
      "291166/291166 [==============================] - 0s 1us/step\n",
      "Epoch 1/5\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 48.9477\n",
      "Epoch 2/5\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.9003\n",
      "Epoch 3/5\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.6341\n",
      "Epoch 4/5\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.5382\n",
      "Epoch 5/5\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.4816\n",
      "291166/291166 [==============================] - 0s 1us/step\n",
      "Epoch 1/5\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 45.0985\n",
      "Epoch 2/5\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.5829\n",
      "Epoch 3/5\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3737\n",
      "Epoch 4/5\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3006\n",
      "Epoch 5/5\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2441\n",
      "291166/291166 [==============================] - 0s 1us/step\n",
      "Epoch 1/5\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 46.7811\n",
      "Epoch 2/5\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.7662\n",
      "Epoch 3/5\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.4768\n",
      "Epoch 4/5\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3694\n",
      "Epoch 5/5\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3115\n",
      "291166/291166 [==============================] - 0s 1us/step\n",
      "Larger: -24.48 (0.55) MSE\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append( ( 'standardize', StandardScaler() ) )\n",
    "estimators.append( ( 'mlp', KerasRegressor( build_fn=larger_model, epochs=5, batch_size=1000, verbose=True ) ) )\n",
    "pipeline = Pipeline( estimators )\n",
    "kfold = KFold( n_splits=5, random_state=seed )\n",
    "results_larger = cross_val_score( pipeline, X_trips, y_trips, cv=kfold )\n",
    "print( \"Larger: %.2f (%.2f) MSE\" % ( results_larger.mean(), results_larger.std() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: -24.48 (0.55) MSE\n"
     ]
    }
   ],
   "source": [
    "# weird negative value!\n",
    "#https://stackoverflow.com/questions/47346809/negative-result-for-regression-using-keras-and-tensorflow\n",
    "print( \"Larger: %.2f (%.2f) MSE\" % ( results_larger.mean(), results_larger.std() ) )\n",
    "# print( \"Larger, Standardized: mean [%.2f], std [%.2f], max [%.2f], and min[%.2f] RMSE\" % ( np.sqrt( results_larger.mean() ), np.sqrt( results_larger.std() ), np.sqrt( results_larger.max() ), np.sqrt( results_larger.min() ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9478546181160299"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt( abs( results_larger.mean() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define wider and deep model\n",
    "def wider_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add( Dense( int( feature_count * 1.5 ), input_dim=feature_count, kernel_initializer='normal', activation='relu' ) )\n",
    "    model.add( Dense( feature_count // 2, kernel_initializer='normal', activation='relu' ) )\n",
    "    model.add( Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile( loss='mean_squared_error', optimizer='adam' )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018.05.14 18:53\n",
      "Epoch 1/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 44.4229\n",
      "Epoch 2/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.8527\n",
      "Epoch 3/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.6543\n",
      "Epoch 4/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.5800\n",
      "Epoch 5/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.5404\n",
      "Epoch 6/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.5133\n",
      "Epoch 7/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.4822\n",
      "Epoch 8/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.4655\n",
      "Epoch 9/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.4608\n",
      "Epoch 10/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.4488\n",
      "Epoch 11/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.4358\n",
      "Epoch 12/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.4361\n",
      "Epoch 13/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.4305\n",
      "Epoch 14/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.4225\n",
      "Epoch 15/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.4137\n",
      "Epoch 16/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.4132\n",
      "Epoch 17/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.4113\n",
      "Epoch 18/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.4072\n",
      "Epoch 19/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.4026\n",
      "Epoch 20/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.4024\n",
      "Epoch 21/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3982\n",
      "Epoch 22/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.3881\n",
      "Epoch 23/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3901\n",
      "Epoch 24/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3910\n",
      "Epoch 25/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3836\n",
      "Epoch 26/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.3835\n",
      "Epoch 27/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3789\n",
      "Epoch 28/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3756\n",
      "Epoch 29/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3702\n",
      "Epoch 30/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3695\n",
      "Epoch 31/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3688\n",
      "Epoch 32/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3642\n",
      "Epoch 33/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3622\n",
      "Epoch 34/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3637\n",
      "Epoch 35/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3556\n",
      "Epoch 36/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3520\n",
      "Epoch 37/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.3468\n",
      "Epoch 38/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3415\n",
      "Epoch 39/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3415\n",
      "Epoch 40/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.3397\n",
      "Epoch 41/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.3397\n",
      "Epoch 42/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3359\n",
      "Epoch 43/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3382\n",
      "Epoch 44/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.3357\n",
      "Epoch 45/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3304\n",
      "Epoch 46/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3287\n",
      "Epoch 47/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3226\n",
      "Epoch 48/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3253\n",
      "Epoch 49/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3220\n",
      "Epoch 50/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3202\n",
      "Epoch 51/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.3190\n",
      "Epoch 52/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3144\n",
      "Epoch 53/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.3039\n",
      "Epoch 54/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.2951\n",
      "Epoch 55/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2991\n",
      "Epoch 56/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.2895\n",
      "Epoch 57/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2942\n",
      "Epoch 58/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2814\n",
      "Epoch 59/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2873\n",
      "Epoch 60/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2864\n",
      "Epoch 61/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2825\n",
      "Epoch 62/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2784\n",
      "Epoch 63/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2735\n",
      "Epoch 64/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.2766\n",
      "Epoch 65/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2690\n",
      "Epoch 66/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2705\n",
      "Epoch 67/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2662\n",
      "Epoch 68/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.2598\n",
      "Epoch 69/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.2626\n",
      "Epoch 70/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2572\n",
      "Epoch 71/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.2531\n",
      "Epoch 72/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2501\n",
      "Epoch 73/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2475\n",
      "Epoch 74/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2442\n",
      "Epoch 75/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2453\n",
      "Epoch 76/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2399\n",
      "Epoch 77/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2346\n",
      "Epoch 78/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2375\n",
      "Epoch 79/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2320\n",
      "Epoch 80/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2276\n",
      "Epoch 81/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2289\n",
      "Epoch 82/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2260\n",
      "Epoch 83/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2178\n",
      "Epoch 84/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2211\n",
      "Epoch 85/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2137\n",
      "Epoch 86/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2172\n",
      "Epoch 87/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2156\n",
      "Epoch 88/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.2141\n",
      "Epoch 89/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2135\n",
      "Epoch 91/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.2071\n",
      "Epoch 92/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.2037\n",
      "Epoch 93/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.1972\n",
      "Epoch 94/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.1967\n",
      "Epoch 95/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.1954\n",
      "Epoch 96/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.1920\n",
      "Epoch 97/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.1912\n",
      "Epoch 98/100\n",
      "1164664/1164664 [==============================] - 2s 2us/step - loss: 24.1906\n",
      "Epoch 99/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.1862\n",
      "Epoch 100/100\n",
      "1164664/1164664 [==============================] - 3s 2us/step - loss: 24.1839\n",
      "291167/291167 [==============================] - 0s 1us/step\n",
      "Epoch 1/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 44.7050\n",
      "Epoch 2/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.9828\n",
      "Epoch 3/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.7769\n",
      "Epoch 4/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.6871\n",
      "Epoch 5/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.6478\n",
      "Epoch 6/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.6157\n",
      "Epoch 7/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.5834\n",
      "Epoch 8/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.5707\n",
      "Epoch 9/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.5523\n",
      "Epoch 10/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.5439\n",
      "Epoch 11/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.5341\n",
      "Epoch 12/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.5241\n",
      "Epoch 13/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.5222\n",
      "Epoch 14/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.5007\n",
      "Epoch 15/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.5039\n",
      "Epoch 16/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.4961\n",
      "Epoch 17/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.4947\n",
      "Epoch 18/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.4871\n",
      "Epoch 19/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.4786\n",
      "Epoch 20/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.4758\n",
      "Epoch 21/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.4781\n",
      "Epoch 22/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.4581\n",
      "Epoch 23/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.4696\n",
      "Epoch 24/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.4541\n",
      "Epoch 25/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.4460\n",
      "Epoch 26/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.4381\n",
      "Epoch 27/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.4280\n",
      "Epoch 28/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.4276\n",
      "Epoch 29/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.4218\n",
      "Epoch 30/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.4172\n",
      "Epoch 31/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.4151\n",
      "Epoch 32/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.4142\n",
      "Epoch 33/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.4143\n",
      "Epoch 34/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.4117\n",
      "Epoch 35/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.4010\n",
      "Epoch 36/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3927\n",
      "Epoch 37/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3968\n",
      "Epoch 38/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3948\n",
      "Epoch 39/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3900\n",
      "Epoch 40/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3876\n",
      "Epoch 41/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3835\n",
      "Epoch 42/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3846\n",
      "Epoch 43/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3839\n",
      "Epoch 44/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3838\n",
      "Epoch 45/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3679\n",
      "Epoch 46/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3700\n",
      "Epoch 47/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3794\n",
      "Epoch 48/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3655\n",
      "Epoch 49/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3670\n",
      "Epoch 50/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3566\n",
      "Epoch 51/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3617\n",
      "Epoch 52/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3572\n",
      "Epoch 53/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3623\n",
      "Epoch 54/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3526\n",
      "Epoch 55/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3567\n",
      "Epoch 56/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3517\n",
      "Epoch 57/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3616\n",
      "Epoch 58/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3500\n",
      "Epoch 59/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3504\n",
      "Epoch 60/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3458\n",
      "Epoch 61/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3444\n",
      "Epoch 62/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3397\n",
      "Epoch 63/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3464\n",
      "Epoch 64/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3377\n",
      "Epoch 65/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3310\n",
      "Epoch 66/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3263\n",
      "Epoch 67/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3382\n",
      "Epoch 68/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3324\n",
      "Epoch 69/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3262\n",
      "Epoch 70/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3244\n",
      "Epoch 71/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3288\n",
      "Epoch 72/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3202\n",
      "Epoch 73/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3340\n",
      "Epoch 74/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3250\n",
      "Epoch 75/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3299\n",
      "Epoch 76/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3173\n",
      "Epoch 77/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3197\n",
      "Epoch 78/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3089\n",
      "Epoch 79/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3334\n",
      "Epoch 80/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3092\n",
      "Epoch 81/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3129\n",
      "Epoch 82/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3117\n",
      "Epoch 83/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3128\n",
      "Epoch 84/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3048\n",
      "Epoch 85/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3121\n",
      "Epoch 86/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3092\n",
      "Epoch 87/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2970\n",
      "Epoch 88/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3015\n",
      "Epoch 89/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3059\n",
      "Epoch 90/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2963\n",
      "Epoch 91/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2956\n",
      "Epoch 92/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.2955\n",
      "Epoch 93/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.2898\n",
      "Epoch 94/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.2960\n",
      "Epoch 95/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3184\n",
      "Epoch 96/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2950\n",
      "Epoch 97/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2826\n",
      "Epoch 98/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2821\n",
      "Epoch 99/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2960\n",
      "Epoch 100/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2946\n",
      "291166/291166 [==============================] - 0s 1us/step\n",
      "Epoch 1/100\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 46.5470\n",
      "Epoch 2/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.8823\n",
      "Epoch 3/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.6690\n",
      "Epoch 4/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.5671\n",
      "Epoch 5/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.5154\n",
      "Epoch 6/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.4765\n",
      "Epoch 7/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.4456\n",
      "Epoch 8/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.4182\n",
      "Epoch 9/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3904\n",
      "Epoch 10/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3784\n",
      "Epoch 11/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3602\n",
      "Epoch 12/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3515\n",
      "Epoch 13/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3254\n",
      "Epoch 14/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3346\n",
      "Epoch 15/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3008\n",
      "Epoch 16/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3048\n",
      "Epoch 17/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2970\n",
      "Epoch 18/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2865\n",
      "Epoch 19/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2810\n",
      "Epoch 20/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2755\n",
      "Epoch 21/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2649\n",
      "Epoch 22/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2559\n",
      "Epoch 23/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.2535\n",
      "Epoch 24/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2441\n",
      "Epoch 25/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.2460\n",
      "Epoch 26/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2335\n",
      "Epoch 27/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2303\n",
      "Epoch 28/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2214\n",
      "Epoch 29/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2118\n",
      "Epoch 30/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2040\n",
      "Epoch 31/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2063\n",
      "Epoch 32/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1960\n",
      "Epoch 33/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1900\n",
      "Epoch 34/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1942\n",
      "Epoch 35/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1825\n",
      "Epoch 36/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.1824\n",
      "Epoch 37/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.1722\n",
      "Epoch 38/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.1812\n",
      "Epoch 39/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.1891\n",
      "Epoch 40/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1546\n",
      "Epoch 41/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1544\n",
      "Epoch 42/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1518\n",
      "Epoch 43/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1449\n",
      "Epoch 44/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1408\n",
      "Epoch 45/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1516\n",
      "Epoch 46/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1306\n",
      "Epoch 47/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1230\n",
      "Epoch 48/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.1295\n",
      "Epoch 49/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1221\n",
      "Epoch 50/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1402\n",
      "Epoch 51/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1126\n",
      "Epoch 52/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1533\n",
      "Epoch 53/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1209\n",
      "Epoch 54/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1141\n",
      "Epoch 55/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1147\n",
      "Epoch 56/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1000\n",
      "Epoch 57/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1168\n",
      "Epoch 58/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1077\n",
      "Epoch 59/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1063\n",
      "Epoch 60/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1275\n",
      "Epoch 61/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1022\n",
      "Epoch 62/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1269\n",
      "Epoch 63/100\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 24.0914\n",
      "Epoch 64/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1120\n",
      "Epoch 65/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1014\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1010\n",
      "Epoch 67/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0834\n",
      "Epoch 68/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1465\n",
      "Epoch 69/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1242\n",
      "Epoch 70/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0814\n",
      "Epoch 71/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0862\n",
      "Epoch 72/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0820\n",
      "Epoch 73/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0852\n",
      "Epoch 74/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1105\n",
      "Epoch 75/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0923\n",
      "Epoch 76/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0824\n",
      "Epoch 77/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0860\n",
      "Epoch 78/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0851\n",
      "Epoch 79/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0929\n",
      "Epoch 80/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.0870\n",
      "Epoch 81/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0861\n",
      "Epoch 82/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.0779\n",
      "Epoch 83/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0702\n",
      "Epoch 84/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.0835\n",
      "Epoch 85/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0619\n",
      "Epoch 86/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1285\n",
      "Epoch 87/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0925\n",
      "Epoch 88/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0389\n",
      "Epoch 89/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0457\n",
      "Epoch 90/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0425\n",
      "Epoch 91/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0618\n",
      "Epoch 92/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0688\n",
      "Epoch 93/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0801\n",
      "Epoch 94/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0459\n",
      "Epoch 95/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0414\n",
      "Epoch 96/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0752\n",
      "Epoch 97/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0423\n",
      "Epoch 98/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.0598\n",
      "Epoch 99/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0451\n",
      "Epoch 100/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0541\n",
      "291166/291166 [==============================] - 0s 1us/step\n",
      "Epoch 1/100\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 44.6655\n",
      "Epoch 2/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.6120\n",
      "Epoch 3/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3565\n",
      "Epoch 4/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2716\n",
      "Epoch 5/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2402\n",
      "Epoch 6/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1954\n",
      "Epoch 7/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1812\n",
      "Epoch 8/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1648\n",
      "Epoch 9/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.1403\n",
      "Epoch 10/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.1275\n",
      "Epoch 11/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1357\n",
      "Epoch 12/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1208\n",
      "Epoch 13/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1187\n",
      "Epoch 14/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1139\n",
      "Epoch 15/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1091\n",
      "Epoch 16/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.1034\n",
      "Epoch 17/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1025\n",
      "Epoch 18/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1058\n",
      "Epoch 19/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0983\n",
      "Epoch 20/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0882\n",
      "Epoch 21/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0818\n",
      "Epoch 22/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0863\n",
      "Epoch 23/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0865\n",
      "Epoch 24/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0650\n",
      "Epoch 25/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0710\n",
      "Epoch 26/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0642\n",
      "Epoch 27/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0608\n",
      "Epoch 28/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0524\n",
      "Epoch 29/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0480\n",
      "Epoch 30/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0493\n",
      "Epoch 31/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0534\n",
      "Epoch 32/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0476\n",
      "Epoch 33/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0374\n",
      "Epoch 34/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.0405\n",
      "Epoch 35/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0251\n",
      "Epoch 36/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0137\n",
      "Epoch 37/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0144\n",
      "Epoch 38/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0090\n",
      "Epoch 39/100\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 24.0102\n",
      "Epoch 40/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9992\n",
      "Epoch 41/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9962\n",
      "Epoch 42/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9979\n",
      "Epoch 43/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 23.9931\n",
      "Epoch 44/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9939\n",
      "Epoch 45/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9880\n",
      "Epoch 46/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9816\n",
      "Epoch 47/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9748\n",
      "Epoch 48/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9773\n",
      "Epoch 49/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9747\n",
      "Epoch 50/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9800\n",
      "Epoch 51/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9628\n",
      "Epoch 52/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9688\n",
      "Epoch 53/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9577\n",
      "Epoch 54/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9616\n",
      "Epoch 55/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9584\n",
      "Epoch 56/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9515\n",
      "Epoch 57/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9537\n",
      "Epoch 58/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9538\n",
      "Epoch 59/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9443\n",
      "Epoch 60/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9431\n",
      "Epoch 61/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9513\n",
      "Epoch 62/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9317\n",
      "Epoch 63/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9319\n",
      "Epoch 64/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9313\n",
      "Epoch 65/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9257\n",
      "Epoch 66/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9400\n",
      "Epoch 67/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9269\n",
      "Epoch 68/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9306\n",
      "Epoch 69/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9294\n",
      "Epoch 70/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9316\n",
      "Epoch 71/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9108\n",
      "Epoch 72/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9207\n",
      "Epoch 73/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9089\n",
      "Epoch 74/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9214\n",
      "Epoch 75/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9102\n",
      "Epoch 76/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.8980\n",
      "Epoch 77/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 23.9120\n",
      "Epoch 78/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9003\n",
      "Epoch 79/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9064\n",
      "Epoch 80/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9046\n",
      "Epoch 81/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9171\n",
      "Epoch 82/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9059\n",
      "Epoch 83/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.8956\n",
      "Epoch 84/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9068\n",
      "Epoch 85/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.8935\n",
      "Epoch 86/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.8893\n",
      "Epoch 87/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.8911\n",
      "Epoch 88/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.8881\n",
      "Epoch 89/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.8880\n",
      "Epoch 90/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.8909\n",
      "Epoch 91/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.8789\n",
      "Epoch 92/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.8897\n",
      "Epoch 93/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.8848\n",
      "Epoch 94/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.8744\n",
      "Epoch 95/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.8781\n",
      "Epoch 96/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 23.8846\n",
      "Epoch 97/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 23.8752\n",
      "Epoch 98/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.8836\n",
      "Epoch 99/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.8687\n",
      "Epoch 100/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 23.8584\n",
      "291166/291166 [==============================] - 0s 2us/step\n",
      "Epoch 1/100\n",
      "1164665/1164665 [==============================] - 3s 3us/step - loss: 45.6589\n",
      "Epoch 2/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.6170\n",
      "Epoch 3/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.4031\n",
      "Epoch 4/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.3373\n",
      "Epoch 5/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.3000\n",
      "Epoch 6/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2703\n",
      "Epoch 7/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2581\n",
      "Epoch 8/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2346\n",
      "Epoch 9/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.2212\n",
      "Epoch 10/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.2070\n",
      "Epoch 11/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1946\n",
      "Epoch 12/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1938\n",
      "Epoch 13/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1775\n",
      "Epoch 14/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1687\n",
      "Epoch 15/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1683\n",
      "Epoch 16/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1631\n",
      "Epoch 17/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1525\n",
      "Epoch 18/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1499\n",
      "Epoch 19/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1514\n",
      "Epoch 20/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1456\n",
      "Epoch 21/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1440\n",
      "Epoch 22/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.1371\n",
      "Epoch 23/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1270\n",
      "Epoch 24/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1335\n",
      "Epoch 25/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1281\n",
      "Epoch 26/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1197\n",
      "Epoch 27/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1166\n",
      "Epoch 28/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1095\n",
      "Epoch 29/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0966\n",
      "Epoch 30/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.1021\n",
      "Epoch 31/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0985\n",
      "Epoch 32/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0901\n",
      "Epoch 33/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0836\n",
      "Epoch 34/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0816\n",
      "Epoch 35/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0757\n",
      "Epoch 36/100\n",
      "1164665/1164665 [==============================] - 2s 2us/step - loss: 24.0732\n",
      "Epoch 37/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0699\n",
      "Epoch 38/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0689\n",
      "Epoch 39/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0675\n",
      "Epoch 40/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0546\n",
      "Epoch 41/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0544\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0525\n",
      "Epoch 43/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0530\n",
      "Epoch 44/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0416\n",
      "Epoch 45/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0439\n",
      "Epoch 46/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0268\n",
      "Epoch 47/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0237\n",
      "Epoch 48/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0210\n",
      "Epoch 49/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0182\n",
      "Epoch 50/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0166\n",
      "Epoch 51/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0079\n",
      "Epoch 52/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0065\n",
      "Epoch 53/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0004\n",
      "Epoch 54/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0042\n",
      "Epoch 55/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0127\n",
      "Epoch 56/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9990\n",
      "Epoch 57/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 24.0143\n",
      "Epoch 58/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9878\n",
      "Epoch 59/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9841\n",
      "Epoch 60/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9857\n",
      "Epoch 61/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9796\n",
      "Epoch 62/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9823\n",
      "Epoch 63/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9859\n",
      "Epoch 64/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9755\n",
      "Epoch 65/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9685\n",
      "Epoch 66/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9727\n",
      "Epoch 67/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9878\n",
      "Epoch 68/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9733\n",
      "Epoch 69/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9685\n",
      "Epoch 70/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9710\n",
      "Epoch 71/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9645\n",
      "Epoch 72/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9799\n",
      "Epoch 73/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9545\n",
      "Epoch 74/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9584\n",
      "Epoch 75/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9597\n",
      "Epoch 76/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9729\n",
      "Epoch 77/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9497\n",
      "Epoch 78/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9561\n",
      "Epoch 79/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9510\n",
      "Epoch 80/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9987\n",
      "Epoch 81/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9568\n",
      "Epoch 82/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9459\n",
      "Epoch 83/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9483\n",
      "Epoch 84/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9519\n",
      "Epoch 85/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9439\n",
      "Epoch 86/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9806\n",
      "Epoch 87/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9561\n",
      "Epoch 88/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9490\n",
      "Epoch 89/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9411\n",
      "Epoch 90/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9684\n",
      "Epoch 91/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9372\n",
      "Epoch 92/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9350\n",
      "Epoch 93/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9426\n",
      "Epoch 94/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9339\n",
      "Epoch 95/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9313\n",
      "Epoch 96/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9389\n",
      "Epoch 97/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9316\n",
      "Epoch 98/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9519\n",
      "Epoch 99/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9607\n",
      "Epoch 100/100\n",
      "1164665/1164665 [==============================] - 3s 2us/step - loss: 23.9135\n",
      "291166/291166 [==============================] - 0s 1us/step\n",
      "Wider: -24.27 (0.48) MSE\n",
      "2018.05.14 19:16\n",
      "Time to process: [1324.4367389678955] seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = get_time()\n",
    "\n",
    "np.random.seed( seed )\n",
    "estimators = []\n",
    "estimators.append( ( 'standardize', StandardScaler() ) )\n",
    "estimators.append( ( 'mlp', KerasRegressor( build_fn=wider_model, epochs=100, batch_size=1000, verbose=True ) ) )\n",
    "pipeline = Pipeline( estimators )\n",
    "kfold = KFold( n_splits=5, random_state=seed )\n",
    "results_wider = cross_val_score( pipeline, X_trips, y_trips, cv=kfold )\n",
    "print( \"Wider: %.2f (%.2f) MSE\" % ( results_wider.mean(), results_wider.std() ) )\n",
    "\n",
    "print_time( start_time, get_time() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.940723891780662"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original vales\n",
    "np.sqrt( abs( results.mean() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9263345413124737"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt( abs( results_wider.mean() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIll They Tip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train & test sets: omit any mention of tips other than y value: 'tip_recorded'\n",
    "X_tips = trips.drop( columns=[ \"tip_amount\", \"tip_percent\", \"total_amount\", \"tip_recorded\", \"tip_percent_bin\" ] )\n",
    "y_tips = trips.tip_recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_count = X_tips.shape[ 1 ]\n",
    "feature_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_classifier():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add( Dense( feature_count, input_dim=feature_count, kernel_initializer='normal', activation='relu' ) )\n",
    "    model.add( Dense( 1, kernel_initializer='normal', activation='sigmoid' ) )\n",
    "    model.compile( loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'] )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018.06.03 14:39\n",
      "Epoch 1/5\n",
      " 516608/1164664 [============>.................] - ETA: 1:11 - loss: 0.2461 - acc: 0.9216"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-141e9cf853d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_baseline_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresults_baseline_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tips\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tips\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"Baseline: %.2f%% (%.2f%%)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mresults_baseline_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_baseline_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    319\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             return_times=True)\n\u001b[0;32m--> 195\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid shape for y: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1192\u001b[0m                             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m                             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m                         raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_slice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = get_time()\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier( build_fn=create_baseline_classifier, epochs=5, batch_size=512, verbose=True )\n",
    "kfold = StratifiedKFold( n_splits=5, shuffle=True, random_state=seed )\n",
    "results_baseline_classifier = cross_val_score( estimator, X_tips, y_tips, cv=kfold )\n",
    "print( \"Baseline: %.2f%% (%.2f%%)\" % ( results_baseline_classifier.mean() * 100, results_baseline_classifier.std() * 100 ) )\n",
    "\n",
    "print_time( start_time, get_time(), interval=\"minutes\" )\n",
    "\n",
    "# 2018.05.25 16:22\n",
    "# Baseline: 93.57% (0.05%)\n",
    "# Time to process: [29.06] minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1164664/1164664 [==============================] - 8s 7us/step - loss: 0.1762 - acc: 0.9353\n",
      "Epoch 2/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1731 - acc: 0.9357\n",
      "Epoch 3/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1722 - acc: 0.9360\n",
      "Epoch 4/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1714 - acc: 0.9361\n",
      "Epoch 5/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1710 - acc: 0.9362\n",
      "291167/291167 [==============================] - 1s 3us/step\n",
      "Epoch 1/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1758 - acc: 0.9355\n",
      "Epoch 2/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1733 - acc: 0.9357\n",
      "Epoch 3/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1726 - acc: 0.9359\n",
      "Epoch 4/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1716 - acc: 0.9361\n",
      "Epoch 5/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1711 - acc: 0.9361\n",
      "291167/291167 [==============================] - 1s 5us/step\n",
      "Epoch 1/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1757 - acc: 0.9356\n",
      "Epoch 2/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1733 - acc: 0.9356\n",
      "Epoch 3/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1723 - acc: 0.9360\n",
      "Epoch 4/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1717 - acc: 0.9361\n",
      "Epoch 5/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1713 - acc: 0.9362\n",
      "291166/291166 [==============================] - 1s 3us/step\n",
      "Epoch 1/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1759 - acc: 0.9357\n",
      "Epoch 2/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1734 - acc: 0.9356\n",
      "Epoch 3/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1727 - acc: 0.9359\n",
      "Epoch 4/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1721 - acc: 0.9360\n",
      "Epoch 5/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1715 - acc: 0.9362\n",
      "291166/291166 [==============================] - 1s 4us/step\n",
      "Epoch 1/5\n",
      "1164666/1164666 [==============================] - 7s 6us/step - loss: 0.1763 - acc: 0.9353\n",
      "Epoch 2/5\n",
      "1164666/1164666 [==============================] - 7s 6us/step - loss: 0.1731 - acc: 0.9358\n",
      "Epoch 3/5\n",
      "1164666/1164666 [==============================] - 7s 6us/step - loss: 0.1720 - acc: 0.9361\n",
      "Epoch 4/5\n",
      "1164666/1164666 [==============================] - 7s 6us/step - loss: 0.1712 - acc: 0.9362\n",
      "Epoch 5/5\n",
      "1164666/1164666 [==============================] - 7s 6us/step - loss: 0.1707 - acc: 0.9362\n",
      "291165/291165 [==============================] - 1s 4us/step\n",
      "Standardized: 93.60% (0.02%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate baseline model with standardized dataset\n",
    "np.random.seed( seed )\n",
    "estimators = []\n",
    "estimators.append( ( 'standardize', StandardScaler() ) )\n",
    "estimators.append( ( 'mlp', KerasClassifier( build_fn=create_baseline_classifier, epochs=5, batch_size=512, verbose=True ) ) )\n",
    "pipeline = Pipeline( estimators )\n",
    "kfold = StratifiedKFold( n_splits=5, shuffle=True, random_state=seed )\n",
    "results_standardized_classifier = cross_val_score( pipeline, X_tips, y_tips, cv=kfold )\n",
    "print( \"Standardized: %.2f%% (%.2f%%)\" % ( results_standardized_classifier.mean() * 100, results_standardized_classifier.std() * 100 ) )\n",
    "\n",
    "#Standardized: 93.60% (0.02%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1774 - acc: 0.9355\n",
      "Epoch 2/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1735 - acc: 0.9356\n",
      "Epoch 3/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1730 - acc: 0.9357\n",
      "Epoch 4/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1721 - acc: 0.9360\n",
      "Epoch 5/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1713 - acc: 0.9362\n",
      "291167/291167 [==============================] - 1s 4us/step\n",
      "Epoch 1/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1772 - acc: 0.9356\n",
      "Epoch 2/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1735 - acc: 0.9357\n",
      "Epoch 3/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1727 - acc: 0.9360\n",
      "Epoch 4/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1719 - acc: 0.9361\n",
      "Epoch 5/5\n",
      "1164664/1164664 [==============================] - 7s 6us/step - loss: 0.1715 - acc: 0.9361\n",
      "291167/291167 [==============================] - 1s 4us/step\n",
      "Epoch 1/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1769 - acc: 0.9357\n",
      "Epoch 2/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1734 - acc: 0.9356\n",
      "Epoch 3/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1729 - acc: 0.9356\n",
      "Epoch 4/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1725 - acc: 0.9358\n",
      "Epoch 5/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1715 - acc: 0.9361\n",
      "291166/291166 [==============================] - 1s 3us/step\n",
      "Epoch 1/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1772 - acc: 0.9355\n",
      "Epoch 2/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1734 - acc: 0.9356\n",
      "Epoch 3/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1730 - acc: 0.9357\n",
      "Epoch 4/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1723 - acc: 0.9359\n",
      "Epoch 5/5\n",
      "1164665/1164665 [==============================] - 7s 6us/step - loss: 0.1716 - acc: 0.9360\n",
      "291166/291166 [==============================] - 1s 3us/step\n",
      "Epoch 1/5\n",
      "1164666/1164666 [==============================] - 7s 6us/step - loss: 0.1766 - acc: 0.9357\n",
      "Epoch 2/5\n",
      "1164666/1164666 [==============================] - 7s 6us/step - loss: 0.1731 - acc: 0.9357\n",
      "Epoch 3/5\n",
      "1164666/1164666 [==============================] - 7s 6us/step - loss: 0.1725 - acc: 0.9358\n",
      "Epoch 4/5\n",
      "1164666/1164666 [==============================] - 7s 6us/step - loss: 0.1719 - acc: 0.9359\n",
      "Epoch 5/5\n",
      "1164666/1164666 [==============================] - 7s 6us/step - loss: 0.1714 - acc: 0.9360\n",
      "291165/291165 [==============================] - 1s 3us/step\n",
      "Smaller: 93.61% (0.03%)\n"
     ]
    }
   ],
   "source": [
    "# make less wide?!?\n",
    "def create_smaller():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add( Dense( feature_count // 2, input_dim=feature_count, kernel_initializer='normal', activation='relu' ) )\n",
    "    model.add( Dense( 1, kernel_initializer='normal', activation='sigmoid' ) )\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'] )\n",
    "    \n",
    "    return model\n",
    "\n",
    "estimators = []\n",
    "estimators.append( ( 'standardize', StandardScaler() ) )\n",
    "estimators.append( ( 'mlp', KerasClassifier(build_fn=create_smaller, epochs=5, batch_size=512, verbose=True ) ) )\n",
    "pipeline = Pipeline( estimators )\n",
    "kfold = StratifiedKFold( n_splits=5, shuffle=True, random_state=seed )\n",
    "results_less_wide_classifier = cross_val_score( pipeline, X_tips, y_tips, cv=kfold )\n",
    "print(\"Smaller: %.2f%% (%.2f%%)\" % ( results_less_wide_classifier.mean() * 100, results_less_wide_classifier.std() * 100 ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip out Tipped Fares from All Fares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(587882, 32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tipped = trips[ trips.tip_recorded == True ]\n",
    "tipped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given Only Tips, Can We Create a More Accurate Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(587882, 28)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tipped = tipped.drop( columns=[ \"tip_amount\", \"tip_percent\", \"tip_percent_bin\", \"total_amount\", \"tip_recorded\" ] )\n",
    "y_tipped = tipped.tip_percent\n",
    "X_tipped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_count = X_tipped.shape[ 1 ]\n",
    "feature_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "470305/470305 [==============================] - 3s 7us/step - loss: 85.6216\n",
      "Epoch 2/10\n",
      "470305/470305 [==============================] - 3s 6us/step - loss: 62.9592\n",
      "Epoch 3/10\n",
      "470305/470305 [==============================] - 3s 6us/step - loss: 61.2690\n",
      "Epoch 4/10\n",
      "470305/470305 [==============================] - 3s 6us/step - loss: 60.7912\n",
      "Epoch 5/10\n",
      "470305/470305 [==============================] - 3s 6us/step - loss: 60.5781\n",
      "Epoch 6/10\n",
      "470305/470305 [==============================] - 3s 6us/step - loss: 60.4517\n",
      "Epoch 7/10\n",
      "470305/470305 [==============================] - 3s 6us/step - loss: 60.3558\n",
      "Epoch 8/10\n",
      "470305/470305 [==============================] - 3s 6us/step - loss: 60.3203\n",
      "Epoch 9/10\n",
      "470305/470305 [==============================] - 3s 6us/step - loss: 60.2281\n",
      "Epoch 10/10\n",
      "470305/470305 [==============================] - 3s 6us/step - loss: 60.1690\n",
      "117577/117577 [==============================] - 0s 2us/step\n",
      "Epoch 1/10\n",
      "470305/470305 [==============================] - 3s 7us/step - loss: 89.9425\n",
      "Epoch 2/10\n",
      "470305/470305 [==============================] - 3s 6us/step - loss: 63.7876\n",
      "Epoch 3/10\n",
      "470305/470305 [==============================] - 3s 6us/step - loss: 62.4817\n",
      "Epoch 4/10\n",
      "470305/470305 [==============================] - 3s 6us/step - loss: 62.0605\n",
      "Epoch 5/10\n",
      "470305/470305 [==============================] - 3s 6us/step - loss: 61.8642\n",
      "Epoch 6/10\n",
      "470305/470305 [==============================] - 3s 6us/step - loss: 61.7486\n",
      "Epoch 7/10\n",
      "470305/470305 [==============================] - 3s 6us/step - loss: 61.6292\n",
      "Epoch 8/10\n",
      "470305/470305 [==============================] - 3s 6us/step - loss: 61.5948\n",
      "Epoch 9/10\n",
      "470305/470305 [==============================] - 3s 6us/step - loss: 61.5335\n",
      "Epoch 10/10\n",
      "470305/470305 [==============================] - 3s 6us/step - loss: 61.4647\n",
      "117577/117577 [==============================] - 0s 2us/step\n",
      "Epoch 1/10\n",
      "470306/470306 [==============================] - 3s 7us/step - loss: 88.8358\n",
      "Epoch 2/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 64.7693\n",
      "Epoch 3/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 62.7427\n",
      "Epoch 4/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.8205\n",
      "Epoch 5/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.5190\n",
      "Epoch 6/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.3422\n",
      "Epoch 7/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.2238\n",
      "Epoch 8/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.1371\n",
      "Epoch 9/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.0865\n",
      "Epoch 10/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.0347\n",
      "117576/117576 [==============================] - 0s 2us/step\n",
      "Epoch 1/10\n",
      "470306/470306 [==============================] - 3s 7us/step - loss: 88.9352\n",
      "Epoch 2/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 64.5877\n",
      "Epoch 3/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 64.0906\n",
      "Epoch 4/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 62.8392\n",
      "Epoch 5/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 62.0302\n",
      "Epoch 6/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.7166\n",
      "Epoch 7/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.4713\n",
      "Epoch 8/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.3552\n",
      "Epoch 9/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.2470\n",
      "Epoch 10/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.1739\n",
      "117576/117576 [==============================] - 0s 2us/step\n",
      "Epoch 1/10\n",
      "470306/470306 [==============================] - 3s 7us/step - loss: 88.8814\n",
      "Epoch 2/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 62.8986\n",
      "Epoch 3/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 62.1538\n",
      "Epoch 4/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.9077\n",
      "Epoch 5/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.7408\n",
      "Epoch 6/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.6390\n",
      "Epoch 7/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.4933\n",
      "Epoch 8/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.4131\n",
      "Epoch 9/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.3778\n",
      "Epoch 10/10\n",
      "470306/470306 [==============================] - 3s 6us/step - loss: 61.3529\n",
      "117576/117576 [==============================] - 0s 2us/step\n",
      "Baseline Tips Only Results: -60.97 (1.72) MSE\n"
     ]
    }
   ],
   "source": [
    "def baseline_tips_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add( Dense( feature_count, input_dim=feature_count, kernel_initializer='normal', activation='relu' ) )\n",
    "    model.add( Dense( 1, kernel_initializer='normal' ) )\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam' )\n",
    "    \n",
    "    return model\n",
    "\n",
    "np.random.seed( seed )\n",
    "estimator = KerasRegressor( build_fn=baseline_tips_model, epochs=10, batch_size=1000, verbose=True )\n",
    "kfold = KFold( n_splits=5, random_state=seed )\n",
    "results_tips_baseline = cross_val_score( estimator, X_tipped, y_tipped, cv=kfold )\n",
    "print(\"Baseline Tips Only Results: %.2f (%.2f) MSE\" % ( results_tips_baseline.mean(), results_tips_baseline.std() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.8082648153739367"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt( abs( results_tips_baseline.mean() ) )\n",
    "# 7.8082648153739367? Let's hope using the zero's from will_tip predictions will counter balance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Multiclass Sequential Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25\n",
       "1     0\n",
       "2    12\n",
       "3     0\n",
       "4     0\n",
       "Name: tip_percent_bin, dtype: int32"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trips = trips.drop( columns=[ \"tip_percent\", \"tip_amount\", \"total_amount\", \"tip_recorded\", \"tip_percent_bin\" ] )\n",
    "y_trips = trips.tip_percent_bin.astype( 'int32' )\n",
    "#y_trips = y_trips.astype( 'int32' )\n",
    "y_trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_count = X_trips.shape[ 1 ]\n",
    "features_count\n",
    "class_count = y_trips.max() + 1\n",
    "class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_76 (Dense)             (None, 340)               58140     \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 170)               57970     \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 85)                14535     \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 130)               11180     \n",
      "=================================================================\n",
      "Total params: 141,825\n",
      "Trainable params: 141,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7f85a41065c0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add( Dense( features_count * 2, input_dim=features_count, kernel_initializer='normal', activation='relu' ) )\n",
    "    #model.add( Dropout( .25 ) )\n",
    "    model.add( Dense( features_count, kernel_initializer='normal', activation='relu' ) )\n",
    "    #model.add( Dropout( .25 ) )\n",
    "    model.add( Dense( features_count // 2, kernel_initializer='normal', activation='relu' ) )\n",
    "    model.add( Dense( class_count, activation='softmax' ) )\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile( loss='categorical_crossentropy', optimizer='adam', metrics=[ 'accuracy' ] )\n",
    "    \n",
    "    #print( model.summary() )\n",
    "    \n",
    "    return model\n",
    "\n",
    "baseline_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_80 (Dense)             (None, 340)               58140     \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 170)               57970     \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 85)                14535     \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 130)               11180     \n",
      "=================================================================\n",
      "Total params: 141,825\n",
      "Trainable params: 141,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "727882/727882 [==============================] - 40s 55us/step - loss: 1.3408 - acc: 0.6615\n",
      "Epoch 2/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.1557 - acc: 0.7072\n",
      "Epoch 3/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.1249 - acc: 0.7150\n",
      "Epoch 4/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.1044 - acc: 0.7201\n",
      "Epoch 5/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0881 - acc: 0.7231\n",
      "Epoch 6/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0745 - acc: 0.7246\n",
      "Epoch 7/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0614 - acc: 0.7269\n",
      "Epoch 8/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0559 - acc: 0.7268\n",
      "Epoch 9/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0440 - acc: 0.7289\n",
      "Epoch 10/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0388 - acc: 0.7293\n",
      "Epoch 11/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0359 - acc: 0.7291\n",
      "Epoch 12/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0303 - acc: 0.7303\n",
      "Epoch 13/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0263 - acc: 0.7310\n",
      "Epoch 14/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0245 - acc: 0.7311\n",
      "Epoch 15/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0204 - acc: 0.7314\n",
      "Epoch 16/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0176 - acc: 0.7316\n",
      "Epoch 17/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0148 - acc: 0.7316\n",
      "Epoch 18/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0114 - acc: 0.7325\n",
      "Epoch 19/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0088 - acc: 0.7324\n",
      "Epoch 20/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0072 - acc: 0.7326\n",
      "Epoch 21/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0047 - acc: 0.7331\n",
      "Epoch 22/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0031 - acc: 0.7330\n",
      "Epoch 23/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 1.0004 - acc: 0.7336\n",
      "Epoch 24/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9987 - acc: 0.7339\n",
      "Epoch 25/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9983 - acc: 0.7335\n",
      "Epoch 26/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9957 - acc: 0.7338\n",
      "Epoch 27/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9947 - acc: 0.7341\n",
      "Epoch 28/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9948 - acc: 0.7339\n",
      "Epoch 29/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9921 - acc: 0.7342\n",
      "Epoch 30/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9900 - acc: 0.7346\n",
      "Epoch 31/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9889 - acc: 0.7349\n",
      "Epoch 32/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9886 - acc: 0.7350\n",
      "Epoch 33/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9873 - acc: 0.7350\n",
      "Epoch 34/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9886 - acc: 0.7345\n",
      "Epoch 35/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9857 - acc: 0.7350\n",
      "Epoch 36/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9843 - acc: 0.7353\n",
      "Epoch 37/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9839 - acc: 0.7351\n",
      "Epoch 38/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9822 - acc: 0.7355\n",
      "Epoch 39/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9825 - acc: 0.7350\n",
      "Epoch 40/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9825 - acc: 0.7351\n",
      "Epoch 41/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9801 - acc: 0.7355\n",
      "Epoch 42/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9800 - acc: 0.7352\n",
      "Epoch 43/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9804 - acc: 0.7352\n",
      "Epoch 44/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9780 - acc: 0.7356\n",
      "Epoch 45/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9778 - acc: 0.7355\n",
      "Epoch 46/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9786 - acc: 0.7355\n",
      "Epoch 47/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9764 - acc: 0.7356\n",
      "Epoch 48/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9774 - acc: 0.7356\n",
      "Epoch 49/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9755 - acc: 0.7357\n",
      "Epoch 50/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9748 - acc: 0.7357\n",
      "Epoch 51/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9752 - acc: 0.7357\n",
      "Epoch 52/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9751 - acc: 0.7356\n",
      "Epoch 53/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9731 - acc: 0.7358\n",
      "Epoch 54/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9738 - acc: 0.7356\n",
      "Epoch 55/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9729 - acc: 0.7358\n",
      "Epoch 56/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9726 - acc: 0.7358\n",
      "Epoch 57/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9740 - acc: 0.7359\n",
      "Epoch 58/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9720 - acc: 0.7358\n",
      "Epoch 59/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9701 - acc: 0.7362\n",
      "Epoch 60/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9707 - acc: 0.7360\n",
      "Epoch 61/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9701 - acc: 0.7363\n",
      "Epoch 62/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9693 - acc: 0.7362\n",
      "Epoch 63/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9699 - acc: 0.7360\n",
      "Epoch 64/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9697 - acc: 0.7361\n",
      "Epoch 65/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9685 - acc: 0.7362\n",
      "Epoch 66/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9680 - acc: 0.7362\n",
      "Epoch 67/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9686 - acc: 0.7362\n",
      "Epoch 68/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9674 - acc: 0.7362\n",
      "Epoch 69/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9670 - acc: 0.7364\n",
      "Epoch 70/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9666 - acc: 0.7365\n",
      "Epoch 71/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9673 - acc: 0.7362\n",
      "Epoch 72/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9668 - acc: 0.7363\n",
      "Epoch 73/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9655 - acc: 0.7365\n",
      "Epoch 74/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9662 - acc: 0.7364\n",
      "Epoch 75/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9665 - acc: 0.7363\n",
      "Epoch 76/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9661 - acc: 0.7362\n",
      "Epoch 77/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9643 - acc: 0.7367\n",
      "Epoch 78/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9655 - acc: 0.7363\n",
      "Epoch 79/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9646 - acc: 0.7365\n",
      "Epoch 80/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9644 - acc: 0.7364\n",
      "Epoch 81/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9657 - acc: 0.7363\n",
      "Epoch 82/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9629 - acc: 0.7367\n",
      "Epoch 83/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9642 - acc: 0.7364\n",
      "Epoch 84/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9637 - acc: 0.7364\n",
      "Epoch 85/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9629 - acc: 0.7366\n",
      "Epoch 86/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9653 - acc: 0.7367\n",
      "Epoch 87/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9631 - acc: 0.7366\n",
      "Epoch 88/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9610 - acc: 0.7368\n",
      "Epoch 89/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9634 - acc: 0.7364\n",
      "Epoch 90/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9629 - acc: 0.7366\n",
      "Epoch 91/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9622 - acc: 0.7368\n",
      "Epoch 92/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9611 - acc: 0.7368\n",
      "Epoch 93/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9620 - acc: 0.7366\n",
      "Epoch 94/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9610 - acc: 0.7367\n",
      "Epoch 95/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9614 - acc: 0.7365\n",
      "Epoch 96/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9613 - acc: 0.7366\n",
      "Epoch 97/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9607 - acc: 0.7368\n",
      "Epoch 98/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9608 - acc: 0.7367\n",
      "Epoch 99/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9596 - acc: 0.7368\n",
      "Epoch 100/100\n",
      "727882/727882 [==============================] - 40s 54us/step - loss: 0.9610 - acc: 0.7366\n",
      "727949/727949 [==============================] - 5s 7us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_84 (Dense)             (None, 340)               58140     \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 170)               57970     \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 85)                14535     \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 130)               11180     \n",
      "=================================================================\n",
      "Total params: 141,825\n",
      "Trainable params: 141,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "727949/727949 [==============================] - 40s 55us/step - loss: 1.3719 - acc: 0.6568\n",
      "Epoch 2/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.1776 - acc: 0.7021\n",
      "Epoch 3/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.1356 - acc: 0.7134\n",
      "Epoch 4/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.1148 - acc: 0.7178\n",
      "Epoch 5/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0964 - acc: 0.7214\n",
      "Epoch 6/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0830 - acc: 0.7229\n",
      "Epoch 7/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0703 - acc: 0.7249\n",
      "Epoch 8/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0622 - acc: 0.7256\n",
      "Epoch 9/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0540 - acc: 0.7270\n",
      "Epoch 10/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0492 - acc: 0.7275\n",
      "Epoch 11/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0439 - acc: 0.7284\n",
      "Epoch 12/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0385 - acc: 0.7296\n",
      "Epoch 13/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0340 - acc: 0.7298\n",
      "Epoch 14/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0306 - acc: 0.7302\n",
      "Epoch 15/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0273 - acc: 0.7307\n",
      "Epoch 16/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0244 - acc: 0.7307\n",
      "Epoch 17/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0191 - acc: 0.7319\n",
      "Epoch 18/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0168 - acc: 0.7318\n",
      "Epoch 19/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0136 - acc: 0.7321\n",
      "Epoch 20/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0110 - acc: 0.7323\n",
      "Epoch 21/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0078 - acc: 0.7330\n",
      "Epoch 22/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0063 - acc: 0.7328\n",
      "Epoch 23/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0046 - acc: 0.7330\n",
      "Epoch 24/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0031 - acc: 0.7330\n",
      "Epoch 25/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 1.0015 - acc: 0.7333\n",
      "Epoch 26/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9994 - acc: 0.7334\n",
      "Epoch 27/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9963 - acc: 0.7340\n",
      "Epoch 28/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9958 - acc: 0.7337\n",
      "Epoch 29/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9943 - acc: 0.7340\n",
      "Epoch 30/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9924 - acc: 0.7345\n",
      "Epoch 31/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9924 - acc: 0.7340\n",
      "Epoch 32/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9906 - acc: 0.7344\n",
      "Epoch 33/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9889 - acc: 0.7346\n",
      "Epoch 34/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9874 - acc: 0.7349\n",
      "Epoch 35/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9880 - acc: 0.7344\n",
      "Epoch 36/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9877 - acc: 0.7345\n",
      "Epoch 37/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9851 - acc: 0.7347\n",
      "Epoch 38/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9846 - acc: 0.7348\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9842 - acc: 0.7348\n",
      "Epoch 40/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9844 - acc: 0.7346\n",
      "Epoch 41/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9820 - acc: 0.7349\n",
      "Epoch 42/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9811 - acc: 0.7349\n",
      "Epoch 43/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9803 - acc: 0.7352\n",
      "Epoch 44/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9794 - acc: 0.7352\n",
      "Epoch 45/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9785 - acc: 0.7354\n",
      "Epoch 46/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9780 - acc: 0.7352\n",
      "Epoch 47/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9798 - acc: 0.7349\n",
      "Epoch 48/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9770 - acc: 0.7354\n",
      "Epoch 49/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9758 - acc: 0.7354\n",
      "Epoch 50/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9770 - acc: 0.7352\n",
      "Epoch 51/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9735 - acc: 0.7357\n",
      "Epoch 52/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9741 - acc: 0.7356\n",
      "Epoch 53/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9754 - acc: 0.7353\n",
      "Epoch 54/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9731 - acc: 0.7357\n",
      "Epoch 55/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9732 - acc: 0.7355\n",
      "Epoch 56/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9733 - acc: 0.7356\n",
      "Epoch 57/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9735 - acc: 0.7355\n",
      "Epoch 58/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9721 - acc: 0.7356\n",
      "Epoch 59/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9726 - acc: 0.7355\n",
      "Epoch 60/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9712 - acc: 0.7359\n",
      "Epoch 61/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9708 - acc: 0.7358\n",
      "Epoch 62/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9699 - acc: 0.7361\n",
      "Epoch 63/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9704 - acc: 0.7359\n",
      "Epoch 64/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9695 - acc: 0.7360\n",
      "Epoch 65/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9692 - acc: 0.7359\n",
      "Epoch 66/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9696 - acc: 0.7358\n",
      "Epoch 67/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9677 - acc: 0.7362\n",
      "Epoch 68/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9676 - acc: 0.7361\n",
      "Epoch 69/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9687 - acc: 0.7358\n",
      "Epoch 70/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9678 - acc: 0.7359\n",
      "Epoch 71/100\n",
      "727949/727949 [==============================] - 40s 54us/step - loss: 0.9688 - acc: 0.7362\n",
      "Epoch 72/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9670 - acc: 0.7361\n",
      "Epoch 73/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9686 - acc: 0.7357\n",
      "Epoch 74/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9670 - acc: 0.7360\n",
      "Epoch 75/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9658 - acc: 0.7361\n",
      "Epoch 76/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9662 - acc: 0.7362\n",
      "Epoch 77/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9662 - acc: 0.7361\n",
      "Epoch 78/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9667 - acc: 0.7359\n",
      "Epoch 79/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9648 - acc: 0.7363\n",
      "Epoch 80/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9652 - acc: 0.7361\n",
      "Epoch 81/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9656 - acc: 0.7362\n",
      "Epoch 82/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9649 - acc: 0.7362\n",
      "Epoch 83/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9647 - acc: 0.7362\n",
      "Epoch 84/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9630 - acc: 0.7364\n",
      "Epoch 85/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9652 - acc: 0.7359\n",
      "Epoch 86/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9627 - acc: 0.7365\n",
      "Epoch 87/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9648 - acc: 0.7360\n",
      "Epoch 88/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9631 - acc: 0.7362\n",
      "Epoch 89/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9639 - acc: 0.7360\n",
      "Epoch 90/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9622 - acc: 0.7363\n",
      "Epoch 91/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9623 - acc: 0.7362\n",
      "Epoch 92/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9615 - acc: 0.7363\n",
      "Epoch 93/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9610 - acc: 0.7365\n",
      "Epoch 94/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9630 - acc: 0.7361\n",
      "Epoch 95/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9621 - acc: 0.7362\n",
      "Epoch 96/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9621 - acc: 0.7363\n",
      "Epoch 97/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9611 - acc: 0.7363\n",
      "Epoch 98/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9623 - acc: 0.7360\n",
      "Epoch 99/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9594 - acc: 0.7366\n",
      "Epoch 100/100\n",
      "727949/727949 [==============================] - 39s 54us/step - loss: 0.9618 - acc: 0.7363\n",
      "727882/727882 [==============================] - 5s 7us/step\n",
      "Baseline: 73.71% (0.04%)\n"
     ]
    }
   ],
   "source": [
    "estimator = KerasClassifier( build_fn=baseline_model, epochs=100, batch_size=512, verbose=1 )\n",
    "#kfold = KFold( n_splits=5, shuffle=True, random_state=seed )\n",
    "kfold = StratifiedKFold( n_splits=2, shuffle=True, random_state=seed )\n",
    "results = cross_val_score( estimator, X_trips, y_trips, cv=kfold )\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % ( results.mean() * 100, results.std() * 100 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
